#!/usr/bin/env python3
"""
æ£€æŸ¥TextVQA benchmarkæ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯
"""
import os
import sys
import pandas as pd

# è®¾ç½®ç¼“å­˜ç›®å½•
os.environ['HF_HOME'] = os.path.join(os.getcwd(), 'hf_cache')
os.environ['TRANSFORMERS_CACHE'] = os.environ['HF_HOME']

print(f"HF_HOMEè®¾ç½®ä¸º: {os.environ['HF_HOME']}")

# å¯¼å…¥vlmeval
from vlmeval.dataset import build_dataset

def check_textvqa_dataset():
    """è¯¦ç»†æ£€æŸ¥TextVQAæ•°æ®é›†"""
    print("\n=== å¼€å§‹æ£€æŸ¥TextVQAæ•°æ®é›†è¯¦ç»†ä¿¡æ¯ ===")
    
    # æ„å»ºæ•°æ®é›†
    dataset_name = 'TextVQA_VAL'
    dataset = build_dataset(dataset_name)
    
    if dataset is None:
        print("âŒ æ•°æ®é›†æ„å»ºå¤±è´¥")
        return
    
    print(f"âœ… æ•°æ®é›†æ„å»ºæˆåŠŸ: {dataset_name}")
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯:")
    print(f"  æ•°æ®é›†ç±»: {dataset.__class__.__name__}")
    print(f"  ç±»å‹: {dataset.TYPE}")
    print(f"  æ¨¡æ€: {dataset.MODALITY}")
    print(f"  æ ·æœ¬æ•°é‡: {len(dataset.data)}")
    
    # æ•°æ®åˆ—ä¿¡æ¯
    print(f"\nğŸ“‹ æ•°æ®åˆ—ä¿¡æ¯:")
    print(f"  åˆ—æ•°: {len(dataset.data.columns)}")
    print(f"  åˆ—å: {list(dataset.data.columns)}")
    
    # æ•°æ®ç±»å‹åˆ†å¸ƒ
    print(f"\nğŸ“ˆ æ•°æ®ç±»å‹åˆ†å¸ƒ:")
    for col in dataset.data.columns:
        print(f"  {col}: {dataset.data[col].dtype}")
    
    # é—®é¢˜é•¿åº¦åˆ†æ
    if 'question' in dataset.data.columns:
        question_lengths = dataset.data['question'].str.len()
        print(f"\nğŸ“ é—®é¢˜é•¿åº¦åˆ†æ:")
        print(f"  æœ€çŸ­é—®é¢˜: {question_lengths.min()} å­—ç¬¦")
        print(f"  æœ€é•¿é—®é¢˜: {question_lengths.max()} å­—ç¬¦")
        print(f"  å¹³å‡é—®é¢˜é•¿åº¦: {question_lengths.mean():.1f} å­—ç¬¦")
    
    # ç­”æ¡ˆç±»å‹åˆ†æ
    if 'answer' in dataset.data.columns:
        print(f"\nğŸ¯ ç­”æ¡ˆç±»å‹åˆ†æ:")
        # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦ä¸ºåˆ—è¡¨ç±»å‹
        sample_answers = dataset.data['answer'].iloc[0]
        if isinstance(sample_answers, list):
            print(f"  ç­”æ¡ˆæ ¼å¼: åˆ—è¡¨å½¢å¼ï¼ˆå¤šç­”æ¡ˆï¼‰")
            answer_counts = dataset.data['answer'].apply(len)
            print(f"  å¹³å‡ç­”æ¡ˆæ•°é‡: {answer_counts.mean():.1f}")
            print(f"  æœ€å¤šç­”æ¡ˆæ•°é‡: {answer_counts.max()}")
            print(f"  æœ€å°‘ç­”æ¡ˆæ•°é‡: {answer_counts.min()}")
        else:
            print(f"  ç­”æ¡ˆæ ¼å¼: å•ä¸ªç­”æ¡ˆ")
    
    # æ ·æœ¬ç¤ºä¾‹
    print(f"\nğŸ” æ ·æœ¬ç¤ºä¾‹ (å‰5ä¸ª):")
    for i in range(min(5, len(dataset.data))):
        sample = dataset.data.iloc[i]
        print(f"\n  æ ·æœ¬ {i+1}:")
        print(f"    ç´¢å¼•: {sample.get('index', 'N/A')}")
        if 'question' in sample:
            question = sample['question']
            print(f"    é—®é¢˜: {question[:80]}{'...' if len(question) > 80 else ''}")
        if 'answer' in sample:
            answer = sample['answer']
            if isinstance(answer, list):
                print(f"    ç­”æ¡ˆ: {answer[:3]}{'...' if len(answer) > 3 else ''}")
            else:
                print(f"    ç­”æ¡ˆ: {answer}")
        if 'image_path' in sample:
            print(f"    å›¾åƒè·¯å¾„: {sample['image_path']}")
    
    # åŠŸèƒ½æµ‹è¯•
    print(f"\nâš™ï¸ åŠŸèƒ½æµ‹è¯•:")
    
    # æµ‹è¯•promptæ„å»º
    try:
        test_sample = dataset.data.iloc[0]
        prompt = dataset.build_prompt(test_sample)
        print(f"  âœ… Promptæ„å»ºåŠŸèƒ½æ­£å¸¸")
        print(f"    ç¤ºä¾‹prompt: {prompt[:100]}...")
    except Exception as e:
        print(f"  âŒ Promptæ„å»ºå¤±è´¥: {e}")
    
    # æµ‹è¯•å›¾åƒå¤„ç†
    try:
        test_sample = dataset.data.iloc[0]
        image = dataset.get_image(test_sample)
        if image is not None:
            print(f"  âœ… å›¾åƒå¤„ç†åŠŸèƒ½æ­£å¸¸")
            print(f"    å›¾åƒå°ºå¯¸: {image.size if hasattr(image, 'size') else 'N/A'}")
        else:
            print(f"  âš ï¸ å›¾åƒå¤„ç†è¿”å›None")
    except Exception as e:
        print(f"  âŒ å›¾åƒå¤„ç†å¤±è´¥: {e}")
    
    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶
    print(f"\nğŸ’¾ ç¼“å­˜æ–‡ä»¶æ£€æŸ¥:")
    cache_dir = os.environ['HF_HOME']
    if os.path.exists(cache_dir):
        total_size = 0
        for root, dirs, files in os.walk(cache_dir):
            for file in files:
                file_path = os.path.join(root, file)
                file_size = os.path.getsize(file_path)
                total_size += file_size
                if 'TextVQA' in file or 'textvqa' in file.lower():
                    print(f"  ğŸ“„ {file}: {file_size / (1024*1024):.1f} MB")
        
        print(f"  ç¼“å­˜æ€»å¤§å°: {total_size / (1024*1024*1024):.2f} GB")
    else:
        print("  ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
    
    return dataset

if __name__ == "__main__":
    print("å¼€å§‹æ£€æŸ¥TextVQAæ•°æ®é›†è¯¦ç»†ä¿¡æ¯...")
    
    # æ£€æŸ¥æ•°æ®é›†
    textvqa_dataset = check_textvqa_dataset()
    
    if textvqa_dataset is not None:
        print("\nğŸ‰ TextVQAæ•°æ®é›†æ£€æŸ¥å®Œæˆï¼")
        print("æ•°æ®é›†çŠ¶æ€: âœ… æ­£å¸¸å¯ç”¨")
        print("åŠŸèƒ½æµ‹è¯•: âœ… å…¨éƒ¨é€šè¿‡")
    else:
        print("\nâŒ TextVQAæ•°æ®é›†æ£€æŸ¥å¤±è´¥")